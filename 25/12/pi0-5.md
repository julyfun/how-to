---
title: "pi0.5"
date: 2025-12-29 00:15:18
tags: ["25", "12"]
author: "Julyfun M4"
os: "Darwin tutianpeikeladeMac-mini.local 24.3.0 Darwin Kernel Version 24.3.0: Thu Jan  2 20:22:58 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8132 arm64"
assume-you-know: [computer]
confidence: 2
---

- [ai, glm4.7]

```
  π₀.₅ (Pi0.5) 模型架构

  基于代码库的分析，π₀.₅ 是一个基于流匹配的视觉-语言-动作模型，是 π₀ 的升级版本，具有更好的开放世界泛化能力。

  核心架构图

  ┌─────────────────────────────────────────────────────────────────────────┐
  │                        π₀.₅ 模型架构 (Pi0.5)                            │
  └─────────────────────────────────────────────────────────────────────────┘

  输入层
  ├─────────────────────────────────────────────────────────────────────┤
  │ 前缀 - 上下文编码                                                     │
  │ ├─ 多视角图像                                                         │
  │ │  ├─ base_0_rgb (224×224×3) - 基础相机视角                        │
  │ │  ├─ left_wrist_0_rgb (224×224×3) - 左腕相机视角                  │
  │ │  └─ right_wrist_0_rgb (224×224×3) - 右腕相机视角                 │
  │ ├─ 语言指令                                                          │
  │ │  └─ tokenized_prompt (最大 200 tokens) - 离散语言标记             │
  │ └─ 状态表示                                                          │
  │    └─ state (32-dim) - **作为离散语言标记处理** (π₀.₅ 特点)         │
  │                                                                       │
  │ 后缀 - 动作生成                                                       │
  │ ├─ 带噪声动作 (noisy_actions) - 流匹配输入                          │
  │ ├─ 时间步嵌入                                                         │
  │ │  └─ timestep ∈ [0.001, 0.999] - 正弦-余弦位置编码                 │
  │ └─ **adaRMSNorm 条件** (π₀.₅ 特点)                                  │
  └─────────────────────────────────────────────────────────────────────┘
                      ↓
  视觉编码器
  ├─────────────────────────────────────────────────────────────────────┤
  │ SigLIP Vision Tower (So400m/14)                                      │
  │ ├─ 2D sincos 位置编码                                                │
  │ ├─ Vision Transformer 处理                                          │
  │ └─ 输出维度: PaliGemma width (取决于 gemma_2b 变体)                 │
  └─────────────────────────────────────────────────────────────────────┘
                      ↓
  双专家语言模型
  ├─────────────────────────────────────────────────────────────────────┤
  │ PaliGemma Dual Experts (基于 Gemma 架构)                           │
  │                                                                       │
  │ 专家 1: PaliGemma 专家 (gemma_2b)                                   │
  │ ├─ 主要语言理解和视觉-语言融合                                       │
  │ ├─ 处理图像和语言输入                                                │
  │ └─ **不使用 adaRMSNorm**                                            │
  │                                                                       │
  │ 专家 2: 动作专家 (gemma_300m)                                       │
  │ ├─ 专门用于动作预测                                                  │
  │ ├─ 使用 **adaRMSNorm** 注入时间步条件 (π₀.₅ 特点)                   │
  │ └─ 接收动作+时间混合表示                                            │
  └─────────────────────────────────────────────────────────────────────┘
                      ↓
  动作输出投影
  ├─────────────────────────────────────────────────────────────────────┤
  │ Action Projection Layer                                             │
  │ ├─ Linear: action_expert.width → action_dim (32)                    │
  │ └─ 输出: 清洁动作 v_t (去噪后的速度场)                              │
  └─────────────────────────────────────────────────────────────────────┘

  π₀.₅ 与 π₀ 的关键区别

  | 特性          | π₀                            | π₀.₅                     |
  |-------------|-------------------------------|--------------------------|
  | 状态输入方式      | 连续状态 token (单独的 suffix token) | 离散语言标记 (作为 prefix 的一部分)  |
  | 时间条件注入      | MLP 混合 (action_time_mlp)      | adaRMSNorm (自适应 RMS 归一化) |
  | 最大 token 长度 | 48                            | 200                      |
  | 动作专家配置      | 不使用 adaRMS                    | 使用 adaRMS 注入流匹配时间步       |
  | 开放世界泛化      | 基线性能                          | 更好 (通过知识隔离训练)            |

  数据流详解

  1. 前缀编码 - embed_prefix()

  # src/openpi/models/pi0.py:106-137
  输入: Observation
  ├─ Images: 3 个相机视角 (224×224×3)
  ├─ Language: tokenized_prompt (最多 200 tokens)
  └─ State: (32-dim) - 在 π₀.₅ 中作为语言标记

  处理流程:
  1. SigLIP 编码每个图像 → image_tokens
  2. PaliGemma LLM 嵌入语言标记 → tokenized_inputs
  3. 图像和语言标记完全注意 (full attention between image and language)
  4. 拼接所有 tokens → prefix_tokens

  2. 后缀编码 - embed_suffix()

  # src/openpi/models/pi0.py:140-186
  输入: noisy_actions, timestep

  处理流程:
  1. Action 投影: Linear(action_dim, width) → action_tokens
  2. 时间嵌入: posemb_sincos(timestep) → time_emb
  3. **时间 MLP** (π₀.₅ 特点):
     - time_mlp_in: Linear(width, width)
     - Swish 激活
     - time_mlp_out: Linear(width, width)
     - Swish 激活
  4. **adaRMS 条件**: adarms_cond = time_emb (用于动作专家)
  5. 动作标记因果自回归 (causal attention)

  3. 流匹配训练

  # src/openpi/models/pi0.py:189-214
  损失计算:
  1. 采样时间步: time ~ Beta(1.5, 1) * 0.999 + 0.001
  2. 构造噪声状态: x_t = time * noise + (1-time) * actions
  3. 计算目标速度场: u_t = noise - actions
  4. 前向传播: v_t = model(x_t, time, observation)
  5. MSE 损失: mean((v_t - u_t)²)

  4. 推理采样

  # src/openpi/models/pi0.py:217-279
  Flow Matching 采样 (ODE Solver):
  1. 初始化: x_T ~ N(0, I), time = 1.0
  2. 迭代去噪 (num_steps 步):
     - 计算速度场: v_t = model(x_t, time)
     - 更新状态: x_{t-1} = x_t - dt * v_t (dt = 1/num_steps)
     - 更新时间: time ← time - dt
  3. 输出: x_0 (预测的动作序列)

  配置示例

  基础配置 (pi0_config.py:18-53):
  Pi0Config(
      dtype="bfloat16",
      paligemma_variant="gemma_2b",        # PaliGemma 专家
      action_expert_variant="gemma_300m",  # 动作专家
      action_dim=32,
      action_horizon=50,
      max_token_len=200,  # π₀.₅: 200, π₀: 48
      pi05=True,  # 启用 π₀.₅ 模式
      discrete_state_input=True,  # 状态作为离散标记
  )

  预训练检查点

  | 模型          | 用途        | 检查点路径                                      |
  |-------------|-----------|--------------------------------------------|
  | π₀.₅-Base   | 微调基础模型    | gs://openpi-assets/checkpoints/pi05_base   |
  | π₀.₅-LIBERO | LIBERO 基准 | gs://openpi-assets/checkpoints/pi05_libero |
  | π₀.₅-DROID  | DROID 数据集 | gs://openpi-assets/checkpoints/pi05_droid  |

  核心创新点

  1. 知识隔离: 通过特殊训练策略提升开放世界泛化
  2. adaRMSNorm: 自适应归一化，将流匹配时间步注入动作专家层
  3. 离散状态表示: 将状态作为语言标记处理，而非连续后缀 token
  4. 双专家架构: 分离语言理解和动作预测，提升专门化性能

  文件位置

  - 配置: src/openpi/models/pi0_config.py:18-110
  - 模型实现: src/openpi/models/pi0.py:66-280
  - PyTorch 版本: src/openpi/models_pytorch/pi0_pytorch.py
  - 训练配置: src/openpi/training/config.py (搜索 pi05_)

  这个架构设计使得 π₀.₅ 在保持 π₀ 的流匹配框架优势的同时，通过更精细的状态表示和条件注入机制实现了更好的泛化性能。
```

```
  | 维度      | π₀               | π₀.₅             |
  |---------|------------------|------------------|
  | 状态表示能力  | 连续向量，1个token     | 离散tokens，多个token |
  | 状态-视觉交互 | ❌ 无交互（单向注意）      | ✅ 完全交互           |
  | 时间条件注入  | MLP混合（token级）    | adaRMS（层级条件）     |
  | 开放世界泛化  | 基线               | 更好               |
  | 训练稳定性   | 需要careful tuning | 更稳定（知识隔离）        |
  | 推理速度    | 相同               | 相同               |
  | 显存占用    | 略少               | 略多（更长序列）         |
```

###

> 简单说说 1. 大家都知道 pi0.5 有预训练和后训练，这两个分别会冻结什么模块吗?
> 2. PaliGemma 是本来就用于 VLM，还是说本来纯粹是一个 LLM 模型 and 预训练也是纯语言 and 只是 pi0 发现 vision token 直接可用？

```
  1. 预训练 vs 后训练 - 会冻结什么模块？

  根据代码分析，取决于训练模式（全量微调 vs LoRA）：

  预训练

  - 全参数训练：所有模块都训练
    - SigLIP Vision Encoder
    - PaliGemma 专家
    - Action Expert (gemma_300m)
    - 所有投影层

  后训练/微调

  有两种策略：

  a) 全量微调 (默认)

  # 示例: pi0_libero, pi05_libero
  model=Pi0Config(pi05=True)  # 无 lora 后缀
  weight_loader=CheckpointWeightLoader("gs://openpi-assets/checkpoints/pi05_base/params")
  # freeze_filter=None → 全部训练
  - 不冻结任何模块
  - 从 base checkpoint 加载全部权重
  - 所有参数继续更新

  b) LoRA 微调 (低内存)

  # 示例: pi0_libero_low_mem_finetune
  model=Pi0Config(
      paligemma_variant="gemma_2b_lora",      # ← LoRA
      action_expert_variant="gemma_300m_lora"  # ← LoRA
  )
  freeze_filter=Pi0Config(...).get_freeze_filter()

  冻结逻辑 (pi0_config.py:80-109):
  def get_freeze_filter(self):
      # 当使用 lora 变体时，冻结主权重，只训练 LoRA 参数

      if "lora" in paligemma_variant:
          冻结: 所有 llm 参数 (.*llm.*)
          排除: lora 参数本身 (.*lora.*)

      if "lora" in action_expert_variant:
          冻结: 动作专家参数 (.*llm.*_1.*)
          排除: lora 参数 (.*lora.*)

  冻结模块总结:

  | 模块                   | 预训练  | 全量微调 | LoRA 微调           |
  |----------------------|------|------|-------------------|
  | SigLIP Vision        | ✅ 训练 | ✅ 训练 | ❌ 冻结              |
  | PaliGemma (gemma_2b) | ✅ 训练 | ✅ 训练 | ❌ 冻结主权重✅ 只训练 LoRA |
  | Action Expert        | ✅ 训练 | ✅ 训练 | ❌ 冻结主权重✅ 只训练 LoRA |
  | 投影层                  | ✅ 训练 | ✅ 训练 | ❌ 冻结              |

  ---
  2. PaliGemma 的来源 - 天生 VLM 还是纯 LLM？

  PaliGemma 本来就是 VLM（Vision-Language Model），并非 π₀ 发现的"意外可用"。

  证据 1: PaliGemma 架构设计

  # pi0.py:73-91
  # PaliGemma = SigLIP + Gemma (已经是 VLM 结构)
  llm = nnx_bridge.ToNNX(_gemma.Module(
      configs=[paligemma_config, action_expert_config],  # ← 双专家
      ...
  ))

  img = nnx_bridge.ToNNX(_siglip.Module(
      num_classes=paligemma_config.width,  # ← 输出给 LLM 的维度
      ...
  ))
  self.PaliGemma = nnx.Dict(llm=llm, img=img)

  PaliGamma 的原始设计:
  - SigLIP (Vision Encoder) → 图像嵌入
  - Gemma (Language Model) → 语言处理
  - 直接拼接 vision tokens + language tokens
  - 跨模态注意力融合

  证据 2: 官方描述

  从 README:
  π₀ 是基于 PaliGemma 的 flow-based VLA

  PaliGemma 是 Google 发布的开源 VLM 模型，专门用于视觉-语言任务。

  π₀ 的创新点在于:

  1. 双专家架构:
    - PaliGemma 专家 (原 VLM 能力)
    - Action Expert (新增，专门预测动作)
  2. 流匹配头:
    - 将 VLM 改造成 VLA
    - 用 flow matching 替代传统的语言建模目标
  3. 并非"发现 vision token 可用":
    - PaliGemma 本来就处理 vision + language
    - π₀ 只是复用了 PaliGemma 的视觉编码和语言理解
    - 然后加了第二个专家专门输出动作

  架构对比:

  原始 PaliGemma (VLM):
  ┌─────────────┐    ┌──────────────┐
  │ SigLIP Img  │───→│ Gemma LLM    │───→ 文本输出
  └─────────────┘    └──────────────┘
         ↑
      Image Input
         +
      Language Tokens

  π₀/π₀.₅ (VLA):
  ┌─────────────┐    ┌──────────────────┬─────────────┐
  │ SigLIP Img  │───→│ PaliGemma Expert │ Action Expert│
  └─────────────┘    └──────────────────┴─────────────┘
                            ↓                    ↓
                       (理解)               (动作输出)

  总结:
  - PaliGemma = 天生 VLM，不是 π₀ 发现的
  - π₀ = VLM → VLA 的改造
  - 核心创新 = 双专家 + 流匹配，而非"视觉 tokens 可用"
```
