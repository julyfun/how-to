- https://www.bilibili.com/video/BV1QxB9YuERU
- RT1
    - Transfromer 出轨迹
    - 3Hz inference
    - 实验：
        - 结合仿真数据训练或者其他机械臂的数据训练，也可以学到新技能.
        - 少一点数据，训练效果会如何变化？
            - 多样性减少的影响更大!
- RT2:
    - Use pretrained VLM. (PaLI-X, PaLM-E)
    - vision language + action Co-finetune.
    - Co-finetune how:
        - Input:
            - ViT
            - Language model
        - Output:
            - sometimes text [err?]
            - sometimes action
        - Inference 的时候只会 sample action token [?]
    - multi-TPU cloud inference
    - 在见过的任务上和 RT1 差不多
    - 没有结果的 task 上面，对世界知识的保留是非常好的.
    - 有 Emergence capability. 可从网络数据中迁移.
    - 参数数量影响结论:
        - Cofinetune 有更好的泛化性
        - 参数量大，泛化性强
    - 引入思维链可以解决很复杂的 command.
- RT-X:
    - 在 OpenX 数据上训练.
    - Trained on 9 manipulators.
    - 小规模数据训练上，超越原有模型.
    - 大规模数据训练上，RT-2-X 效果好.
        - 说明数据量大场景下需要高容量模型结构.
    - 跨具身形态训练，让模型从别的 domain 学到技能
    - 在输入中包含 image history 可以增强 generalization 性能
    - web-scale pretaining 重要 [?]
    - OXE (Open X Emb..) 非常 diverse 
- RT-H:
    - 考虑到有很多任务比较的动作序列比较像.
    - 思想: 加入 Language motion 中间层。输入任务后先预测移动指令（用语言描述）
        - 好处：多任务数据共享
    - Co-train: Internet data, action data & language motion data
    - Language motion 人类很容易干预，检查和纠正. 纠正也可以用于训练，比遥操作纠正效果更好.
    - 在新任务上需要很少的 correction.
    - 启示：lang-motion 可以 scale up 数据收集，可以压缩 action-space. 从而样本高效.
        - 但是对于复杂任务 lang-motion 可能比较复杂.
- OpenVLA
    - 训练数据集更大更 Diverse
    - Didn't use internet data, use only robot data.
    - 模型结构：
        - prethmetic VLM [?]
        - Input to => DINOv2 and SigLIP 融合到一起的 Vision Encoder.
            - Get spacial and high-level semantic feature.
        - outrput => Small 2-layer MLP projector.
            - Project to language token space [?]
        - => Llama2 7B language model
        - => Action De-tokenizer
            - 输出 action. (夹爪）
    - 性能实验:
        - OXE 上训练
        - out-of-the-box 直接泛化能力
        - 做了数据清洗
    - VLA Finetune 性能实验:
        - 发现 OpenVLA 在困难 diverse 多种 instructions 多物体任务上，比原来更好
        - 归结于 OpenX 的 pretraining.
    - Lora 实验:
        - LoRA 在性能保持上非常好！低资源 Finetune
    - 验证 memory-efficient
        - 4bit 量化可以将 GPU memory 减半，并保持差不多的性能
- [下面改进 action output]
    - problem:
        - policy 执行每一步都有可能与误差，会累积.
        - 同一个任务的方法是多峰的
    - 使用 action chunking
    - 使用生成式建模
- [idea] action chunking
    - 每一步我都预测后四步，然后做加权.
- [idea] 生成式建模
- [paper] ACT (Conditional Variational AutoEncoder)
    - ...todo
- [paper] Diffusion Policy
- 表达多峰分布
- high-dimensional, 可建模长 action 序列的分布