---
title: RL 285
date: 2024-07-01 15:26:42
tags:
  - notes
  - julyfun
  - "24"
  - "07"
author: julyfun-4070s-ubuntu2204
os: "Linux julyfun-4070s-ubuntu 6.8.0-87-generic #88~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Oct 14 14:03:14 UTC 2 x86_64 x86_64 x86_64 GNU/Linux"
assume-you-know:
  - computer
confidence: 2
---
https://rail.eecs.berkeley.edu/deeprlcourse-fa23/
- hw1: https://rail.eecs.berkeley.edu/deeprlcourse-fa23/deeprlcourse-fa23/static/homeworks/hw1.pdf
- hw2: 
- 讲师：Sergey Levine https://people.eecs.berkeley.edu/~svlevine/ (some talks here)

## 符号
- $J(theta)$: 策略的累积奖励的期望，需要最大化
- 顺序: $s_1 => a_1 => s_2$
- $tau$: 轨迹，表示所有 $s, a$

- $pi(a | s)$：状态 $s$ 下采取 $a$ 的概率
- $V^pi (s) eq.def EE_pi [G_t | S_t = s]$ **状态价值函数 state-value function**，即还不确定 $a$
- $Q^pi (s, a) = EE_pi [G_t | S_t = s, A_t = a]$ **动作价值函数 action-value function**，即确定了 $a$
    - 有 $V^pi (s) = sum_(a in A) pi(a | s) Q^pi (s, a)$
    - 有 $Q^pi (s, a) = r(s,a) + gamma sum_(s^prime in S) P(s^prime | s, a) V^pi (s^prime)$
    - $s -->^pi a_i -->^P s^prime$

## FAQ
$log$ 何时来的？
- ![|400](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to202511230220374.png)
- 从右往左看.

## 强化学习类型
- Policy Gradient: 求 $EE[r]$ 对 $theta$ 的导数.
	- 训练:
		- Actor: 输入 `[机械臂状态，观测]`
		- 输出 `[动作]` 或者 `[动作的概率分布]`
	- 推理: 一样
- Value Based (DQN): 直接训练一个 Q / V，取最大值对应的动作索引 (no explicit policy)
- Actor-Critic: 有 A 有 Q
- Model-based: 有模型自行估计 $s$ 经过 $a$ 如何转移 ( learn $p(s_(t + 1) | s_t, a_t)$ )

## On-off policy
- off-policy: able to improve the policy without generating new samples from that policy
- on-policy: any time the policy is changed (even a little bit) we need to generate new samples.

- (and there is offline-RL)

## Lec5 Policy Gradients
- https://rail.eecs.berkeley.edu/deeprlcourse-fa23/deeprlcourse-fa23/static/slides/lec-5.pdf
- Maximum likehood 仅仅让 $theta$ 朝着“这批动作出现概率最大”的方向演进.
- ![image.png|600](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116164938.webp)
- ![image.png|650](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116170620.webp)
```python
logits = policy(states)
loss_fn = torch.nn.CrossEntropyLoss()
loss = loss_fn(logits, actions) # 离散动作
loss.backward()
gradients = [p.grad for p in policy.parameters()]
```
- 问题：奖励方差大，训练效率低下。好轨迹梯度可能为 0（累积奖励 0），有效奖励信号丢失. 
### 例子：高斯 policy
这里距离是马氏距离，用协方差使得距离评估更准.
- ![image.png|450](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116183909.webp)

### 两种优化
- 换种形式: reward to go:
    - ![image.png|650](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116171204.webp)
- 等等先换一个话题，我们求一个 baseline $b$ 并改写奖励为 ![image.png|350](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116173030.webp)
，目的是使梯度方差最小。推导出最优的 $b$ 为:
    - ![image.png|650](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116172912.webp)
    - 其中: ![image.png|200](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116172922.webp)
- 结合以上两个优化，得到:
- ![image.png|450](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116173232.webp)

### 为什么 PG 必须是 on-policy
上述公式是对 $theta$ 求导，$theta$  必须是最新的，求的梯度才有意义。导致训练效率很低. 当然你可以多采样几次，相当于 batch 大很多.

### importance sampling
- 这个公式就是 IS: $$E_(x tilde p(x)) [f(x)] = E_(x tilde q(x)) [ p(x) / q(x) f(x) ]$$
- ![image.png|450](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116174112.webp)
- ![1|650](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116180142.webp)
- [ok]
    - [grep] 注意上图 $Pi$ 那个概率在下图这里已经改写为乘积完毕的形式 $pi_(theta^prime) (s_(i,t), a_(i,t ))$.
- ![image.png|650](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116180805.webp)
- 最后还是改用自动求导了，因为显示计算 $log$ 那一项开销太大. 而 $log$ 那一项正好对应平方误差.

### 优化的梯度下降?
- 能直接走到最好的 $theta$吗？
- ![image.png|650](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116211939.webp)
- 上图第一个方法“参数距离约束”依赖于参数的具体形式，不好。
- 上图 KL 散度: ![image.png|350](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116212127.webp) 通过采样计算.
- ![image.png|350](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116212627.webp)
```python
# states: (B, S_dim) -> logits: (B, A_dim)
logits_old = policy_old(states)
logits_new = policy_new(states)

# 将 logits 转换为概率分布. 连续动作用 Normal 代替 Categorical
dist_old = torch.distributions.Categorical(logits=logits_old)
dist_new = torch.distributions.Categorical(logits=logits_new)

# 对所有样本取平均，得到最终的 KL 散度值
kl_divergence = torch.distributions.kl.kl_divergence(dist_new, dist_old).mean()
```
- 进一步：通过 Fisher 信息矩阵近似展开 KL 散度:
- ![image.png|350](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116213038.webp)
- 可以通过采样来估计 $F$.
- ![image.png|350](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116213110.webp)
- WHY this formula? see:
    - ![image.png|550](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116213504.webp)
- ![image.png|350](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116213626.webp)
- 选择 $alpha$: natural gradient. 选择 $epsilon$: trust region policy optimization
- Lec5 end!

## Hw1
- hw1: https://rail.eecs.berkeley.edu/deeprlcourse-fa23/deeprlcourse-fa23/static/homeworks/hw1.pdf
### 1.1 Given
![image.png|550](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251117081152.webp)
show: ![image.png|250](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251117081227.webp)

- see: https://blog.csdn.net/weixin_55471672/article/details/138329230
- 第一题我证半天不会证，给我整笑了.

### 1.2

![image.png|650](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251117091902.webp)
- holy!
- The Off-policy policy gradient: 这张图简单易懂:
- ![image.png|650](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20251116180142.webp)

### 2 Editing Code
如何可视化: `tensorboard --logdir=/home/julyfun/Documents/GitHub/homework_fall2023/hw1/data/q1_bc_ant_HalfCheetah-v4_2
3-11-2025_00-00-06/`
``
- Ant-v4:
```
Eval_AverageReturn : 4795.3828125
Train_AverageReturn : 4681.891673935816
Training Loss : 0.0011749982368201017
```
- Walker-2d 只有 2d 物理
```
Eval_AverageReturn : 998.958740234375
Train_AverageReturn : 5383.310325177668
Training Loss : 0.01655399613082409
```
走的不太行，能往前冲一点
- HalfCheetah
```
Eval_AverageReturn : 4070.5146484375
Train_AverageReturn : 4034.7999834965067
Training Loss : 0.004244370386004448
```
![](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-toScreenshot%20from%202025-11-23%2000-02-08.png)
- Hopper
```
Eval_AverageReturn : 1542.371826171875
Train_AverageReturn : 3717.5129936182307
Training Loss : 0.010719751007854939
```
看视频跳的还可以
![](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-tooutput.gif)
关于这个 Hopper 任务, batchsize: （我本来还想搞 N 条轨迹来实验，但是这个专家数据只有若干个 s-a pair，没法搞）
![|400](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-toreturns_curve.png)

![|400](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-toScreenshot%20from%202025-11-23%2000-40-45.png)

### 3. Dagger
- Walker-2d
- 这里 `batch_size default=1000` 参数不用调，因为 train 的时候只拿出前 `train_batch_size=100` 个. 是公平的.
```
Eval_AverageReturn : 998.958740234375
Eval_StdReturn : 488.8404541015625
Train_AverageReturn : 5383.310325177668
Train_StdReturn : 54.15251563871789
Training Loss : 0.01655399613082409

********** Iteration 1 ************
Eval_AverageReturn : 5408.2431640625
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 1130.504150390625
Train_StdReturn : 583.0662231445312
Training Loss : 0.022556820884346962

********** Iteration 2 ************
Eval_AverageReturn : 5389.76708984375
Eval_StdReturn : 0.0
Train_AverageReturn : 5411.14599609375
Train_StdReturn : 0.0
Training Loss : 0.012993226759135723
```

- 实验时在默认命令上尽量少改动.
- Walker2d DAGGER:
![|400](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to202511230143565.png)

- 强制不收集新数据: `if itr == 0:` => `if itr == 0 or True:` （后来我改了其他代码，可以 no_dagger 多轮执行）
![|400](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to202511230141991.png)
- no-dagger Step 0: 会 terminate
![|400](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to202511230204570.gif)
- DAgger Step 9:
![|400](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to202511230200439.gif)

- Hopper DAgger:
![|400](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to202511230146012.png)
- Hopper no dagger
![|400](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to202511230151875.png)
- DAgger Step 9:
![|400](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to202511230202044.gif)

## Lec6 Actor Critic
- https://www.youtube.com/watch?v=wr00ef_TY6Q&list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps&index=21
- https://rail.eecs.berkeley.edu/deeprlcourse-fa23/deeprlcourse-fa23/static/slides/lec-6.pdf
- 为什么加入 baseline 项可以减少方差呢？
	- 因为 Q(s, a) - V(s) 可以分离动作 $a$ 本身的价值，降低梯度波动.
	- 我的理解是对于完美的 Q 来说减了 V 也不会降低方差了.
- ![|600](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to202511230239813.png)