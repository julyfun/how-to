## Embedding 层

see: https://blog.csdn.net/zhaohongfei_358/article/details/122809709

与独热编码不同，Embedding 可以将词表降维，且可进行学习。也就是做了 word2vec

```py
import torch
from torch import nn

embedding = nn.Embedding(20, 5)
# 输入必须为 long 类型
embedding(torch.LongTensor([0,1,2]))
```

```
# 初始状态为随机编码，会随着梯度下降逐渐学习
tensor([[ 0.4471,  0.3875, -1.0195, -1.1125,  1.3481],
        [-1.7230, -0.1964, -0.0420,  0.5782,  0.4514],
        [-0.0310, -1.9674, -1.1344, -1.6752,  1.0801]],
       grad_fn=<EmbeddingBackward0>)
```

- 类参数 `padding_idx`：指定填充的索引，这个索引的向量会被初始化为 0。

## 不错捏
