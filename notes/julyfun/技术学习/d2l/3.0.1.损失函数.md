## 交叉熵损失 Cross Entropy Loss
$$"Loss" = - sum_(i = 1)^n y_i log limits(y)^tilde_i$$

- $y_i$ 为真实分布，形如 $[0, 0, 1, 0]$. $y_i^tilde$ 为预测分布，形如 $[0.1, 0.1, 0.6, 0.2]$
- 独热编码情况下仅看正确标签的输出与 $1$ 有多接近。每差 2 倍概率，损失 +1
- batch 下乘以 $1 / "batch_size"$

- 也可设计为将错误分类的交叉熵考虑进去 $- sum_(i = 1)^n (y_i log)$$
