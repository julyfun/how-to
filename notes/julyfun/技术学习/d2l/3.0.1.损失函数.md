## 交叉熵损失 Cross Entropy Loss


$$"Loss" = - sum_(i = 1)^n y_i log limits(y_i)^tilde$$

- $y_i$ 为真实分布，形如 $[0, 0, 1, 0]$. $y_i^tilde$ 为预测分布，形如 $[0.1, 0.1, 0.6, 0.2]$
- 独热编码情况下仅看正确标签的输出与 $1$ 有多接近。每差 2 倍概率，损失 +1
- batch 下乘以 $1 / "batch_size"$

- 用于分类问题
- 也可设计为将错误分类的交叉熵考虑进去

$$- sum_(i = 1)^n (y_i log limits(y_i)^tilde + (1 - y_i) log (1 - limits(y_i)^tilde))$$

很好理解吧。

## 均方损失 MSELoss

$$(x_i - y_i)^2$$
