---
title: 7.7.densenet
date: 2024-08-15 16:01:49
tags: []
---
## 概述

densenet 与 resnet 相比，拼接了连续多个 ConvBlock 构成一个 DenseBlock，并且使用的也不是相加而是 `torch.cat`。例如，对于一个 DenseBlock，若输入通道为 64，第一个卷积层输出通道数为 32，则会先拼接原始 64 通道和卷积输出的 32 通道，构成 96 通道输入到第二个卷积层，再输出通道数 32，则拼接为 96 + 32 = 128 通道。

此处的标准 conv_block 变成了 BN + ReLU + Conv2d3x3

为了保持通道数不爆炸，引入过渡块，直接 BN + Conv2d1x1 + AvgPool2x2 把通道数压下来（压一半）。最前面和最后面与 Googlenet 一致。

---

训练时间 2m 36.6s （高宽已经 resize 到 96）

```
loss 0.140, train acc 0.949, test acc 0.903
```

## 练习

1. 为什么我们在过渡层使用平均汇聚层而不是最大汇聚层？
    - 不知道
1. DenseNet的优点之一是其模型参数比ResNet小。为什么呢？
1. DenseNet一个诟病的问题是内存或显存消耗过多。
    1. 真的是这样吗？可以把输入形状换成$224 \times 224$，来看看实际的显存消耗。
    1. 有另一种方法来减少显存消耗吗？需要改变框架么？
1. 实现DenseNet论文 :cite:`Huang.Liu.Van-Der-Maaten.ea.2017`表1所示的不同DenseNet版本。
1. 应用DenseNet的思想设计一个基于多层感知机的模型。将其应用于 :numref:`sec_kaggle_house`中的房价预测任务。
