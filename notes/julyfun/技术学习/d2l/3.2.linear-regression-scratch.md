---
title: 3.2.linear-regression-scratch
date: 2024-08-06 11:17:09
tags: []
---
## 批量大小越大，则梯度下降越快？

(ai) ref: https://www.perplexity.ai/search/zai-pytorch-zhong-ruo-wo-diao-XuETZW8eSoSQFqW3H_kRqw

如果使用 SGD，是的：

```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # X和y的小批量损失
        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，
        # 并以此计算关于[w,b]的梯度
        l.sum().backward()
        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```
