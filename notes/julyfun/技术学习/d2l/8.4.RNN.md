---
title: 8.4.RNN
date: 2024-08-16 11:13:06
tags: ["notes", "julyfun", "技术学习", "d2l"]
---
## RNN 基础模型

$$bold(H)^(in n times h)_t = phi(bold(X^(in n times d "输入维度")) bold(W_(x h)^(in d times h)) + bold(H)_(t - 1)^(in h times h) bold(W)_(h h)^(in h times h) b_h)$$

$$O_t = H_t W_(h q "输出维度") + b_q$$

这里 $W_(x h), W_(h h)$ 极其类似于单隐藏层感知机中的隐藏层，只是前一刻的隐藏层输出会成为下一刻隐藏层的输入的一部分。而 $H_t$ 隐状态则存储在网络之外。

## Perplexity 困惑度

$$exp(- 1 / n sum_(t = 1)^(n) log P(x_t | x_(t - 1), ..., x_1))$$

可以直接利用神经网络输出的概率，评估它有多自信。当每个输出的概率均为 $1$ 时，困惑度为 $1$，当概率为 $0$ 时困惑度正无穷，当概率为均匀分布时困惑度为唯一词元数（也是未压缩情况下存储序列最好的编码方式）。看到一个语言模型报告其 perplexity 为 $109$ 时，直观理解为它每次输出认为有 $109$ 个词作为下一个词的合理选择。ref: http://sentiment-mining.blogspot.com/2016/11/perplexity.html
