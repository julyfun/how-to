## 时序差分算法

- 用于评估价值函数，$alpha$ 为常数，和蒙特卡洛一样是基于采样。优点在于，每采样一步就可以更新状态估计。
- 同样会收敛。

下式中 $t$ 为模拟的时间步。

    $$V(s_t) <- V(s_t) + alpha [r_t + gamma V(s_(t + 1)) - V(s_t)]$$
## Sarsa

- 用于求解最优策略。

- 考虑到时序差分算法也可以用来估计动作价值函数 $Q(s_t, a_t)$
- 以 $1 - epsilon$ 概率采样动作价值最大的动作，剩下概率随机选一个
- 每次采样步更新：$Q(s, a) <- Q(s, a) + alpha [r + gamma Q(s^prime, a^prime) - Q(s, a)]$

```
每个时间步（已知 s 和动作 a）：
    得到环境反馈 r, s'
    e-greedy 选一个 a'
    Q(s, a) <- Q(s, a) + alpha[r + gamma Q(s', a') - Q(s, a)]
    s, a = s', a'
```

核心代码：

```
while not done:
    next_state, reward, done = env.step(action)
    next_action = agent.take_action(next_state)
    agent.update(state, action, reward, next_state, next_action)
    state = next_state
    action = next_action

class Sarsa:
    def update(self, s0, a0, r, s1, a1):
        td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]
        self.Q_table[s0, a0] += self.alpha * td_error
```

## n 步  Sarsa

- 采样到至少 $n$ 步后，对最近 $n$ 步实施类似蒙特卡洛的反向更新。

```py
class Sarsa:
    def update(self, s0, a0, r, s1, a1, done):
        self.state_list.append(s0)
        self.action_list.append(a0)
        self.reward_list.append(r)
        if len(self.state_list) == self.n:  # 若保存的数据可以进行n步更新
            G = self.Q_table[s1, a1]  # 得到Q(s_{t+n}, a_{t+n})
            for i in reversed(range(self.n)):
                G = self.gamma * G + self.reward_list[i]  # 不断向前计算每一步的回报
                # 如果到达终止状态,最后几步虽然长度不够n步,也将其进行更新
                if done and i > 0:
                    s = self.state_list[i]
                    a = self.action_list[i]
                    self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])
            s = self.state_list.pop(0)  # 将需要更新的状态动作从列表中删除,下次不必更新
            a = self.action_list.pop(0)
            self.reward_list.pop(0)
            # n步Sarsa的主要更新步骤
            self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])
        if done:  # 如果到达终止状态,即将开始下一条序列,则将列表全清空
            self.state_list = []
            self.action_list = []
            self.reward_list = []

```

## Q-learning

$$Q(s_t, a_t) <- Q(s_t, a_t) + alpha [r + gamma max_a Q(s_(t + 1), a) - Q(s, a)]$$
