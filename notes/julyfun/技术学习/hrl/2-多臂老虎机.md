## 符号

- $cal(A)$: 动作集合 $cal(R)$: 奖励概率分布，动作 $a$ 对应一个奖励分布 $cal(R)(r | a)$
- 对动作 $a$，定义其期望奖励为 $Q(a)$
- 最优期望奖励 $Q^* = max_(a in cal(A)) Q(a)$
- 懊悔 $R(a) = Q^* - Q(a)$
- $limits(Q)^("hat")$: 对 $a$ 的期望奖励估值

## 名称

- MAB: 多臂老虎机
- UCB: 上置信界法

## $epsilon$ 贪心算法

$$
a_t = cases(
  arg max_(a in cal(A)) Q^"hat" (a) & "采样概率"  1 - epsilon,
  "从" cal(A) "随机选择" & "采样概率" epsilon
)
$$

以  $epsilon$ 的概率随机探索一个。结果由于随机的部分，懊悔是线性增长的。

## 随时间衰减的 $epsilon$ 贪心算法

$epsilon_t = 1 / t$

测试时 $K = 10$（老虎机个数），结果累计懊悔是 $ln$ 形式增长的。

![image.png](https://how-to-1258460161.cos.ap-shanghai.myqcloud.com/how-to/20240926210423.webp)

## 上置信界算法

在 https://hrl.boyuai.com/chapter/1/%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/#25-%E4%B8%8A%E7%BD%AE%E4%BF%A1%E7%95%8C%E7%AE%97%E6%B3%95 中已经最小化讲清楚了。

用到了霍夫丁不等式。每一时刻设一个概率 $p = 1 / t$。对于每个动作 $a$ 算出一个 $u$ s.t. $p = e^(-2N_t(a)U_t(a)^2)$，根据霍夫丁不等式必有 $Q_t (a) < Q^"hat"_t (a) + u$

